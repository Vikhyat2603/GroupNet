{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "sys.path.append(os.getcwd())\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataloader_dance import DanceDataset, seq_collate\n",
    "from model.GroupNet_dance import GroupNet\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Training parameters\n",
    "        self.seed = 1\n",
    "        self.dataset = 'dance'\n",
    "        self.wandb_enabled = True  # Set to True to enable WandB logging\n",
    "        self.wandb_project = 'dance-generation'  # WandB project name\n",
    "        self.wandb_entity = 'vikhyat3'  # Your WandB username\n",
    "        self.batch_size = 64 # batch size\n",
    "        self.past_length = 8 # number of frames to condition on\n",
    "        self.future_length = 10 # number of frames to predict into the future \n",
    "        self.traj_scale = 1 # scale factor applied to trajectory coordinates\n",
    "        self.learn_prior = False # whether to learn prior distribution vs using fixed prior\n",
    "        self.lr = 3e-4 # learning rate\n",
    "        self.weight_decay = 0.0001\n",
    "        self.sample_k = 20 # number of samples to generate during testing for diverse predictions\n",
    "        self.num_epochs = 5 # MAKE 100\n",
    "        self.decay_step = 10 # number of epochs before applying learning rate decay\n",
    "        self.decay_gamma = 0.9 # learning rate decay factor\n",
    "        self.print_every_it = 18 # print training stats every N iterations\n",
    "        # self.test_every_it = 27 # test model every N iterations\n",
    "\n",
    "        # Model parameters\n",
    "        self.ztype = 'gaussian' # type of latent distribution: 'gaussian' or 'vmf'\n",
    "        self.zdim = 32 # dimension of latent variable\n",
    "        self.hidden_dim = 64 # dimension of hidden layers\n",
    "        self.hyper_scales = [15,53] # scales for hyperprior ([5,11] for nba)\n",
    "        self.num_decompose = 2 # number of decomposed distributions\n",
    "        self.min_clip = 2.0\n",
    "\n",
    "        # Save/load parameters\n",
    "        self.model_save_dir = 'saved_models/dance'\n",
    "        self.model_save_epoch = 5 # save model every N epochs\n",
    "        self.epoch_continue = 0 # epoch to continue training from, 0 if training from scratch\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-04-22_20-44-27'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "{'seed': 1, 'dataset': 'dance', 'wandb_enabled': True, 'wandb_project': 'dance-generation', 'wandb_entity': 'vikhyat3', 'batch_size': 64, 'past_length': 8, 'future_length': 10, 'traj_scale': 1, 'learn_prior': False, 'lr': 0.0003, 'weight_decay': 0.0001, 'sample_k': 20, 'num_epochs': 5, 'decay_step': 10, 'decay_gamma': 0.9, 'print_every_it': 18, 'ztype': 'gaussian', 'zdim': 32, 'hidden_dim': 64, 'hyper_scales': [15, 53], 'num_decompose': 2, 'min_clip': 2.0, 'model_save_dir': 'saved_models/dance', 'model_save_epoch': 5, 'epoch_continue': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikhyat3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: <ipython-input-5-b1f881d4516b> 18 <module>\n",
      "Warning: Could not initialize WandB: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 403: Forbidden)\n",
      "Training will continue without WandB logging\n"
     ]
    }
   ],
   "source": [
    "\"\"\" setup \"\"\"\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device:',device)\n",
    "print(args)\n",
    "\n",
    "# Initialize WandB\n",
    "run = None\n",
    "if args.wandb_enabled:\n",
    "    try:\n",
    "        run = wandb.init(\n",
    "            project=args.wandb_project,\n",
    "            entity=args.wandb_entity,\n",
    "            config=args.__dict__,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not initialize WandB: {e}\")\n",
    "        print(\"Training will continue without WandB logging\")\n",
    "        args.wandb_enabled = False\n",
    "\n",
    "if not os.path.isdir(args.model_save_dir):\n",
    "    os.makedirs(args.model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, epoch):\n",
    "    model.train()\n",
    "    total_iter_num = len(train_loader)\n",
    "    iter_num = 0\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        total_loss, loss_pred, loss_recover, loss_kl, loss_diverse = model(data)\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        \"\"\" optimize \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter_num % args.print_every_it == 0:\n",
    "            print('Epochs: {:02d}/{:02d}| It: {:04d}/{:04d} | Train loss: {:6.3f} (pred: {:6.3f}| recover: {:6.3f}| kl: {:6.3f}| diverse: {:6.3f})'\n",
    "                    .format(epoch,args.num_epochs,iter_num,total_iter_num,total_loss.item(),loss_pred,loss_recover,loss_kl,loss_diverse))\n",
    "            # Log metrics to WandB if enabled\n",
    "            if args.wandb_enabled:\n",
    "                wandb.log({\n",
    "                    'train_loss': total_loss.item(),\n",
    "                    'pred_loss': loss_pred,\n",
    "                    'recover_loss': loss_recover,\n",
    "                    'kl_loss': loss_kl,\n",
    "                    'diverse_loss': loss_diverse,\n",
    "                    'epoch': epoch,\n",
    "                    'iteration': iter_num\n",
    "                })\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_loss = epoch_loss / total_iter_num\n",
    "    print(f'Epoch {epoch} completed in {epoch_time:.2f} seconds. Average loss: {avg_loss:.3f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    model.step_annealer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 18, 53, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.load('datasets/dance/train.npy') \n",
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ct, data_test in enumerate(test_loader):\n",
    "#     print(model(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train data with shape: (10, 18, 53, 3)\n",
      "10\n",
      "Loaded Test data with shape: (5, 18, 53, 3)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model & optimizer \"\"\"\n",
    "model = GroupNet(args,device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.decay_step, gamma=args.decay_gamma)\n",
    "\n",
    "train_size = 10\n",
    "\n",
    "\"\"\" dataloader \"\"\"\n",
    "train_set = DanceDataset(\n",
    "    obs_len=args.past_length,\n",
    "    pred_len=args.future_length,\n",
    "    n_samples=train_size,\n",
    "    training=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=seq_collate,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_set = DanceDataset(\n",
    "    obs_len=args.past_length,\n",
    "    pred_len=args.future_length,\n",
    "    n_samples=5,\n",
    "    training=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=len(test_set),\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=seq_collate,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([53, 8, 3]), torch.Size([53, 10, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train_set[0]\n",
    "train_sample[0].shape, train_sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading if needed \"\"\"\n",
    "if args.epoch_continue > 0:\n",
    "    checkpoint_path = os.path.join(args.model_save_dir,str(args.epoch_continue)+'.p')\n",
    "    print('load model from: {checkpoint_path}')\n",
    "    model_load = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(model_load['model_dict'])\n",
    "    if 'optimizer' in model_load:\n",
    "        optimizer.load_state_dict(model_load['optimizer'])\n",
    "    if 'scheduler' in model_load:\n",
    "        scheduler.load_state_dict(model_load['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_encoder.input_fc.weight torch.Size([64, 6])\n",
      "past_encoder.input_fc.bias torch.Size([64])\n",
      "past_encoder.input_fc2.weight torch.Size([64, 512])\n",
      "past_encoder.input_fc2.bias torch.Size([64])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([6, 128])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([6])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "past_encoder.interaction.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "past_encoder.interaction.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "past_encoder.interaction.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "past_encoder.interaction.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "past_encoder.interaction.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "past_encoder.interaction.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "past_encoder.interaction.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "past_encoder.interaction.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.spatial_embedding.weight torch.Size([64, 2])\n",
      "past_encoder.interaction_hyper.spatial_embedding.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.spatial_transform.weight torch.Size([64, 64])\n",
      "past_encoder.interaction_hyper.spatial_transform.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([10, 128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([10])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "past_encoder.interaction_hyper.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "past_encoder.interaction_hyper.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "past_encoder.interaction_hyper.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction_hyper.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "past_encoder.interaction_hyper.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "past_encoder.interaction_hyper.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "past_encoder.interaction_hyper.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.spatial_embedding.weight torch.Size([64, 2])\n",
      "past_encoder.interaction_hyper2.spatial_embedding.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.spatial_transform.weight torch.Size([64, 64])\n",
      "past_encoder.interaction_hyper2.spatial_transform.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([10, 128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([10])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "past_encoder.interaction_hyper2.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "past_encoder.interaction_hyper2.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "past_encoder.interaction_hyper2.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "past_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "past_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "past_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "past_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.1.bias torch.Size([64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "past_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "past_encoder.pos_encoder.fc.weight torch.Size([64, 128])\n",
      "past_encoder.pos_encoder.fc.bias torch.Size([64])\n",
      "pz_layer.weight torch.Size([64, 256])\n",
      "pz_layer.bias torch.Size([64])\n",
      "future_encoder.input_fc.weight torch.Size([64, 6])\n",
      "future_encoder.input_fc.bias torch.Size([64])\n",
      "future_encoder.input_fc2.weight torch.Size([64, 640])\n",
      "future_encoder.input_fc2.bias torch.Size([64])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([6, 128])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([6])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "future_encoder.interaction.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "future_encoder.interaction.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "future_encoder.interaction.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "future_encoder.interaction.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "future_encoder.interaction.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "future_encoder.interaction.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "future_encoder.interaction.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "future_encoder.interaction.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.spatial_embedding.weight torch.Size([16, 2])\n",
      "future_encoder.interaction_hyper.spatial_embedding.bias torch.Size([16])\n",
      "future_encoder.interaction_hyper.spatial_transform.weight torch.Size([64, 64])\n",
      "future_encoder.interaction_hyper.spatial_transform.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([10, 128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([10])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "future_encoder.interaction_hyper.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "future_encoder.interaction_hyper.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "future_encoder.interaction_hyper.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction_hyper.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "future_encoder.interaction_hyper.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "future_encoder.interaction_hyper.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "future_encoder.interaction_hyper.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.6.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.7.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.8.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.agg_mlp.9.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.spatial_embedding.weight torch.Size([16, 2])\n",
      "future_encoder.interaction_hyper2.spatial_embedding.bias torch.Size([16])\n",
      "future_encoder.interaction_hyper2.spatial_transform.weight torch.Size([64, 64])\n",
      "future_encoder.interaction_hyper2.spatial_transform.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.1.weight torch.Size([10, 128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_distribution.layers.1.bias torch.Size([10])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.1.weight torch.Size([1, 128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.MLP_factor.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_start.init_MLP.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_end.layers.0.weight torch.Size([128, 128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_end.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_end.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.nmp_mlp_end.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.attention_mlp.0.layers.0.weight torch.Size([32, 128])\n",
      "future_encoder.interaction_hyper2.attention_mlp.0.layers.0.bias torch.Size([32])\n",
      "future_encoder.interaction_hyper2.attention_mlp.0.layers.1.weight torch.Size([1, 32])\n",
      "future_encoder.interaction_hyper2.attention_mlp.0.layers.1.bias torch.Size([1])\n",
      "future_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.0.weight torch.Size([256, 64])\n",
      "future_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.0.bias torch.Size([256])\n",
      "future_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.1.weight torch.Size([64, 256])\n",
      "future_encoder.interaction_hyper2.node2edge_start_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.0.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.1.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.2.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.3.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.4.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.5.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.6.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.7.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.8.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.agg_mlp.9.layers.1.bias torch.Size([64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.0.weight torch.Size([128, 64])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.0.bias torch.Size([128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.1.weight torch.Size([64, 128])\n",
      "future_encoder.interaction_hyper2.edge_aggregation_list.0.mlp.layers.1.bias torch.Size([64])\n",
      "future_encoder.pos_encoder.fc.weight torch.Size([64, 128])\n",
      "future_encoder.pos_encoder.fc.bias torch.Size([64])\n",
      "future_encoder.out_mlp.affine_layers.0.weight torch.Size([128, 512])\n",
      "future_encoder.out_mlp.affine_layers.0.bias torch.Size([128])\n",
      "future_encoder.qz_layer.weight torch.Size([64, 128])\n",
      "future_encoder.qz_layer.bias torch.Size([64])\n",
      "decoder.decompose.0.conv_past.weight torch.Size([32, 3, 3])\n",
      "decoder.decompose.0.conv_past.bias torch.Size([32])\n",
      "decoder.decompose.0.encoder_past.weight_ih_l0 torch.Size([288, 32])\n",
      "decoder.decompose.0.encoder_past.weight_hh_l0 torch.Size([288, 96])\n",
      "decoder.decompose.0.encoder_past.bias_ih_l0 torch.Size([288])\n",
      "decoder.decompose.0.encoder_past.bias_hh_l0 torch.Size([288])\n",
      "decoder.decompose.0.decoder_y.layers.0.weight torch.Size([512, 384])\n",
      "decoder.decompose.0.decoder_y.layers.0.bias torch.Size([512])\n",
      "decoder.decompose.0.decoder_y.layers.1.weight torch.Size([256, 512])\n",
      "decoder.decompose.0.decoder_y.layers.1.bias torch.Size([256])\n",
      "decoder.decompose.0.decoder_y.layers.2.weight torch.Size([30, 256])\n",
      "decoder.decompose.0.decoder_y.layers.2.bias torch.Size([30])\n",
      "decoder.decompose.0.decoder_x.layers.0.weight torch.Size([512, 384])\n",
      "decoder.decompose.0.decoder_x.layers.0.bias torch.Size([512])\n",
      "decoder.decompose.0.decoder_x.layers.1.weight torch.Size([256, 512])\n",
      "decoder.decompose.0.decoder_x.layers.1.bias torch.Size([256])\n",
      "decoder.decompose.0.decoder_x.layers.2.weight torch.Size([24, 256])\n",
      "decoder.decompose.0.decoder_x.layers.2.bias torch.Size([24])\n",
      "decoder.decompose.1.conv_past.weight torch.Size([32, 3, 3])\n",
      "decoder.decompose.1.conv_past.bias torch.Size([32])\n",
      "decoder.decompose.1.encoder_past.weight_ih_l0 torch.Size([288, 32])\n",
      "decoder.decompose.1.encoder_past.weight_hh_l0 torch.Size([288, 96])\n",
      "decoder.decompose.1.encoder_past.bias_ih_l0 torch.Size([288])\n",
      "decoder.decompose.1.encoder_past.bias_hh_l0 torch.Size([288])\n",
      "decoder.decompose.1.decoder_y.layers.0.weight torch.Size([512, 384])\n",
      "decoder.decompose.1.decoder_y.layers.0.bias torch.Size([512])\n",
      "decoder.decompose.1.decoder_y.layers.1.weight torch.Size([256, 512])\n",
      "decoder.decompose.1.decoder_y.layers.1.bias torch.Size([256])\n",
      "decoder.decompose.1.decoder_y.layers.2.weight torch.Size([30, 256])\n",
      "decoder.decompose.1.decoder_y.layers.2.bias torch.Size([30])\n",
      "decoder.decompose.1.decoder_x.layers.0.weight torch.Size([512, 384])\n",
      "decoder.decompose.1.decoder_x.layers.0.bias torch.Size([512])\n",
      "decoder.decompose.1.decoder_x.layers.1.weight torch.Size([256, 512])\n",
      "decoder.decompose.1.decoder_x.layers.1.bias torch.Size([256])\n",
      "decoder.decompose.1.decoder_x.layers.2.weight torch.Size([24, 256])\n",
      "decoder.decompose.1.decoder_x.layers.2.bias torch.Size([24])\n",
      "Total number of parameters: 3156108\n"
     ]
    }
   ],
   "source": [
    "# show all parameter shapes for GroupNet, and count total number of parameters\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "\tprint(name, param.data.shape)\n",
    "\ttotal_params += np.prod(param.data.shape)\n",
    "print('Total number of parameters:', total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/va6hp/GroupNet/model/MS_HGNN_batch.py:476: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 00/05| It: 0000/0001 | Train loss: 19.629 (pred:  1.573| recover: 15.814| kl:  2.000| diverse:  0.243)\n",
      "Epoch 0 completed in 0.57 seconds. Average loss: 19.629\n",
      "Epochs: 01/05| It: 0000/0001 | Train loss: 16.309 (pred:  0.991| recover: 13.167| kl:  2.000| diverse:  0.151)\n",
      "Epoch 1 completed in 0.33 seconds. Average loss: 16.309\n",
      "Epochs: 02/05| It: 0000/0001 | Train loss: 13.782 (pred:  0.743| recover: 10.927| kl:  2.000| diverse:  0.112)\n",
      "Epoch 2 completed in 0.36 seconds. Average loss: 13.782\n",
      "Epochs: 03/05| It: 0000/0001 | Train loss: 11.807 (pred:  0.701| recover:  9.000| kl:  2.000| diverse:  0.106)\n",
      "Epoch 3 completed in 0.34 seconds. Average loss: 11.807\n",
      "Epochs: 04/05| It: 0000/0001 | Train loss: 10.083 (pred:  0.770| recover:  7.199| kl:  2.000| diverse:  0.113)\n",
      "Epoch 4 completed in 0.34 seconds. Average loss: 10.083\n"
     ]
    }
   ],
   "source": [
    "\"\"\" start training \"\"\"\n",
    "model.set_device(device)\n",
    "for epoch in range(args.epoch_continue, args.num_epochs):\n",
    "    train(train_loader,epoch)\n",
    "    \"\"\" save model \"\"\"\n",
    "    if  (epoch + 1) % args.model_save_epoch == 0:\n",
    "        model_saved = {'model_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'scheduler': scheduler.state_dict(), 'epoch': epoch + 1,'model_cfg': args}\n",
    "        saved_path = os.path.join(args.model_save_dir,str(epoch+1)+'.p')\n",
    "        torch.save(model_saved, saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "##################\n",
      "ADE 1.0s: 0.032786876894533634\n",
      "ADE 2.0s: 0.05441300570964813\n",
      "ADE 3.0s: 0.06626186519861221\n",
      "ADE 4.0s: 0.07543409615755081\n",
      "FDE 1.0s: 0.025778746232390404\n",
      "FDE 2.0s: 0.05313604697585106\n",
      "FDE 3.0s: 0.06723129749298096\n",
      "FDE 4.0s: 0.0723644569516182\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "all_num = 0\n",
    "l2error_overall = 0\n",
    "l2error_dest = 0\n",
    "l2error_avg_04s = 0\n",
    "l2error_dest_04s = 0\n",
    "l2error_avg_08s = 0\n",
    "l2error_dest_08s = 0\n",
    "l2error_avg_12s = 0\n",
    "l2error_dest_12s = 0\n",
    "l2error_avg_16s = 0\n",
    "l2error_dest_16s = 0\n",
    "l2error_avg_20s = 0\n",
    "l2error_dest_20s = 0\n",
    "l2error_avg_24s = 0\n",
    "l2error_dest_24s = 0\n",
    "l2error_avg_28s = 0\n",
    "l2error_dest_28s = 0\n",
    "l2error_avg_32s = 0\n",
    "l2error_dest_32s = 0\n",
    "l2error_avg_36s = 0\n",
    "l2error_dest_36s = 0\n",
    "\n",
    "for data in test_loader:\n",
    "\tfuture_traj = np.array(data['future_traj']) * args.traj_scale # B,N,T,2\n",
    "\twith torch.no_grad():\n",
    "\t\tprediction = model.inference(data)\n",
    "\tprediction = prediction * args.traj_scale\n",
    "\tprediction = np.array(prediction.cpu()) #(BN,20,T,2)\n",
    "\tbatch = future_traj.shape[0]\n",
    "\tactor_num = future_traj.shape[1]\n",
    "\n",
    "\ty = np.reshape(future_traj,(batch*actor_num,args.future_length, 3))\n",
    "\ty = y[None].repeat(20,axis=0)\n",
    "\tl2error_avg_04s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:1,:] - prediction[:,:,:1,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_04s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,0:1,:] - prediction[:,:,0:1,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_08s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:2,:] - prediction[:,:,:2,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_08s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,1:2,:] - prediction[:,:,1:2,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_12s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:3,:] - prediction[:,:,:3,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_12s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,2:3,:] - prediction[:,:,2:3,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_16s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:4,:] - prediction[:,:,:4,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_16s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,3:4,:] - prediction[:,:,3:4,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_20s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:5,:] - prediction[:,:,:5,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_20s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,4:5,:] - prediction[:,:,4:5,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_24s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:6,:] - prediction[:,:,:6,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_24s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,5:6,:] - prediction[:,:,5:6,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_28s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:7,:] - prediction[:,:,:7,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_28s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,6:7,:] - prediction[:,:,6:7,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_32s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:8,:] - prediction[:,:,:8,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_32s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,7:8,:] - prediction[:,:,7:8,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_avg_36s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:9,:] - prediction[:,:,:9,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest_36s += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,8:9,:] - prediction[:,:,8:9,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_overall += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,:10,:] - prediction[:,:,:10,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tl2error_dest += np.mean(np.min(np.mean(np.linalg.norm(y[:,:,9:10,:] - prediction[:,:,9:10,:], axis = 3),axis=2),axis=0))*batch\n",
    "\tall_num += batch\n",
    "\n",
    "print(all_num)\n",
    "l2error_overall /= all_num\n",
    "l2error_dest /= all_num\n",
    "\n",
    "l2error_avg_04s /= all_num\n",
    "l2error_dest_04s /= all_num\n",
    "l2error_avg_08s /= all_num\n",
    "l2error_dest_08s /= all_num\n",
    "l2error_avg_12s /= all_num\n",
    "l2error_dest_12s /= all_num\n",
    "l2error_avg_16s /= all_num\n",
    "l2error_dest_16s /= all_num\n",
    "l2error_avg_20s /= all_num\n",
    "l2error_dest_20s /= all_num\n",
    "l2error_avg_24s /= all_num\n",
    "l2error_dest_24s /= all_num\n",
    "l2error_avg_28s /= all_num\n",
    "l2error_dest_28s /= all_num\n",
    "l2error_avg_32s /= all_num\n",
    "l2error_dest_32s /= all_num\n",
    "l2error_avg_36s /= all_num\n",
    "l2error_dest_36s /= all_num\n",
    "print('##################')\n",
    "print('ADE 1.0s:',(l2error_avg_08s+l2error_avg_12s)/2)\n",
    "print('ADE 2.0s:',l2error_avg_20s)\n",
    "print('ADE 3.0s:',(l2error_avg_32s+l2error_avg_28s)/2)\n",
    "print('ADE 4.0s:',l2error_overall)\n",
    "\n",
    "print('FDE 1.0s:',(l2error_dest_08s+l2error_dest_12s)/2)\n",
    "print('FDE 2.0s:',l2error_dest_20s)\n",
    "print('FDE 3.0s:',(l2error_dest_28s+l2error_dest_32s)/2)\n",
    "print('FDE 4.0s:',l2error_dest)\n",
    "print('##################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7950.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "265*10*2*3/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
