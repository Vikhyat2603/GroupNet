{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "sys.path.append(os.getcwd())\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataloader_dance import DanceDataset, seq_collate\n",
    "from model.GroupNet_dance import GroupNet\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Training parameters\n",
    "        self.seed = 1\n",
    "        self.dataset = 'dance'\n",
    "        self.batch_size = 32\n",
    "        self.past_length = 8 # number of frames to condition on\n",
    "        self.future_length = 10 # number of frames to predict into the future \n",
    "        self.traj_scale = 1 # scale factor applied to trajectory coordinates\n",
    "        self.learn_prior = False # whether to learn prior distribution vs using fixed prior\n",
    "        self.lr = 3e-5 # learning rate\n",
    "        self.weight_decay = 0.001\n",
    "        self.sample_k = 20 # number of samples to generate during testing for diverse predictions\n",
    "        self.num_epochs = 100\n",
    "        self.decay_step = 10 # number of epochs before applying learning rate decay\n",
    "        self.decay_gamma = 0.9 # learning rate decay factor\n",
    "        self.print_every_it = 18 # print training stats every N iterations\n",
    "        self.test_every_it = 27 # test model every N iterations\n",
    "\n",
    "        # Model parameters\n",
    "        self.ztype = 'gaussian' # type of latent distribution: 'gaussian' or 'vmf'\n",
    "        self.zdim = 32 # dimension of latent variable\n",
    "        self.hidden_dim = 64 # dimension of hidden layers\n",
    "        self.hyper_scales = [15,53] # scales for hyperprior ([5,11] for nba)\n",
    "        self.num_decompose = 2 # number of decomposed distributions\n",
    "        self.min_clip = 2.0\n",
    "\n",
    "        # Save/load parameters\n",
    "        self.model_save_dir = 'saved_models/dance'\n",
    "        self.model_save_epoch = 5 # save model every N epochs\n",
    "        self.epoch_continue = 0 # epoch to continue training from, 0 if training from scratch\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "{'seed': 1, 'dataset': 'dance', 'batch_size': 32, 'past_length': 8, 'future_length': 10, 'traj_scale': 1, 'learn_prior': False, 'lr': 3e-05, 'weight_decay': 0.001, 'sample_k': 20, 'num_epochs': 100, 'decay_step': 10, 'decay_gamma': 0.9, 'print_every_it': 18, 'test_every_it': 27, 'ztype': 'gaussian', 'zdim': 32, 'hidden_dim': 64, 'hyper_scales': [15, 53], 'num_decompose': 2, 'min_clip': 2.0, 'model_save_dir': 'saved_models/dance', 'model_save_epoch': 5, 'epoch_continue': 0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" setup \"\"\"\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device:',device)\n",
    "print(args)\n",
    "\n",
    "if not os.path.isdir(args.model_save_dir):\n",
    "    os.makedirs(args.model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,epoch):\n",
    "\tmodel.train()\n",
    "\ttotal_iter_num = len(train_loader)\n",
    "\titer_num = 0\n",
    "\tfor data in train_loader:\n",
    "\t\ttotal_loss, loss_pred, loss_recover, loss_kl, loss_diverse = model(data)\n",
    "\t\t\"\"\" optimize \"\"\"\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ttotal_loss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif iter_num % args.print_every_it == 0:\n",
    "\t\t\tprint('Epochs: {:02d}/{:02d}| It: {:04d}/{:04d} | Train loss: {:6.3f} (pred: {:6.3f}| recover: {:6.3f}| kl: {:6.3f}| diverse: {:6.3f})'\n",
    "\t\t\t\t\t.format(epoch,args.num_epochs,iter_num,total_iter_num,total_loss.item(),loss_pred,loss_recover,loss_kl,loss_diverse))\n",
    "\n",
    "\t\t# if iter_num % args.test_every_it == 0:\n",
    "\t\t# \twith torch.no_grad():\n",
    "\t\t# \t\tmodel.eval()\n",
    "\t\t# \t\ttotal_loss, loss_pred, loss_recover, loss_kl, loss_diverse = model(test_loader)\n",
    "\t\t# \t\tprint('\\t\\t\\t Test  | Test loss: {:6.3f} (pred: {:6.3f}| recover: {:6.3f}| kl: {:6.3f}| diverse: {:6.3f})'\n",
    "\t\t# \t\t\t\t.format(epoch,args.num_epochs,iter_num,total_iter_num,total_loss.item(),loss_pred,loss_recover,loss_kl,loss_diverse))\n",
    "\t\t\n",
    "\t\titer_num += 1\n",
    "\n",
    "\tscheduler.step()\n",
    "\tmodel.step_annealer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 18, 53, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.load('datasets/dance/train.npy') \n",
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ct, data_test in enumerate(test_loader):\n",
    "#     print(model(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Train data with shape: (1702, 18, 53, 2)\n",
      "1702\n",
      "Loaded Test data with shape: (426, 18, 53, 2)\n",
      "426\n"
     ]
    }
   ],
   "source": [
    "\"\"\" model & optimizer \"\"\"\n",
    "model = GroupNet(args,device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.decay_step, gamma=args.decay_gamma)\n",
    "\n",
    "\"\"\" dataloader \"\"\"\n",
    "train_set = DanceDataset(\n",
    "    obs_len=args.past_length,\n",
    "    pred_len=args.future_length,\n",
    "    training=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=seq_collate,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_set = DanceDataset(\n",
    "    obs_len=args.past_length,\n",
    "    pred_len=args.future_length,\n",
    "    training=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=len(test_set),\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=seq_collate,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading if needed \"\"\"\n",
    "if args.epoch_continue > 0:\n",
    "    checkpoint_path = os.path.join(args.model_save_dir,str(args.epoch_continue)+'.p')\n",
    "    print('load model from: {checkpoint_path}')\n",
    "    model_load = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(model_load['model_dict'])\n",
    "    if 'optimizer' in model_load:\n",
    "        optimizer.load_state_dict(model_load['optimizer'])\n",
    "    if 'scheduler' in model_load:\n",
    "        scheduler.load_state_dict(model_load['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/va6hp/GroupNet/model/MS_HGNN_batch.py:476: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 00/100| It: 0000/0054 | Train loss: 16.032 (pred:  0.986| recover: 12.902| kl:  2.000| diverse:  0.145)\n",
      "Epochs: 00/100| It: 0018/0054 | Train loss: 10.707 (pred:  0.647| recover:  7.964| kl:  2.000| diverse:  0.096)\n",
      "Epochs: 00/100| It: 0036/0054 | Train loss:  6.604 (pred:  0.395| recover:  4.154| kl:  2.000| diverse:  0.056)\n",
      "Epochs: 01/100| It: 0000/0054 | Train loss:  4.454 (pred:  0.607| recover:  1.754| kl:  2.000| diverse:  0.092)\n",
      "Epochs: 01/100| It: 0018/0054 | Train loss:  3.899 (pred:  0.469| recover:  1.358| kl:  2.000| diverse:  0.071)\n",
      "Epochs: 01/100| It: 0036/0054 | Train loss:  3.641 (pred:  0.317| recover:  1.279| kl:  2.000| diverse:  0.045)\n",
      "Epochs: 02/100| It: 0000/0054 | Train loss:  3.669 (pred:  0.408| recover:  1.202| kl:  2.000| diverse:  0.059)\n",
      "Epochs: 02/100| It: 0018/0054 | Train loss:  3.635 (pred:  0.452| recover:  1.116| kl:  2.000| diverse:  0.068)\n",
      "Epochs: 02/100| It: 0036/0054 | Train loss:  3.175 (pred:  0.334| recover:  0.794| kl:  2.000| diverse:  0.047)\n",
      "Epochs: 03/100| It: 0000/0054 | Train loss:  2.940 (pred:  0.382| recover:  0.503| kl:  2.000| diverse:  0.055)\n",
      "Epochs: 03/100| It: 0018/0054 | Train loss:  3.041 (pred:  0.375| recover:  0.611| kl:  2.000| diverse:  0.055)\n",
      "Epochs: 03/100| It: 0036/0054 | Train loss:  3.212 (pred:  0.638| recover:  0.474| kl:  2.000| diverse:  0.100)\n",
      "Epochs: 04/100| It: 0000/0054 | Train loss:  2.602 (pred:  0.244| recover:  0.324| kl:  2.000| diverse:  0.034)\n",
      "Epochs: 04/100| It: 0018/0054 | Train loss:  3.157 (pred:  0.697| recover:  0.349| kl:  2.000| diverse:  0.111)\n",
      "Epochs: 04/100| It: 0036/0054 | Train loss:  2.760 (pred:  0.436| recover:  0.258| kl:  2.000| diverse:  0.066)\n",
      "Epochs: 05/100| It: 0000/0054 | Train loss:  2.619 (pred:  0.330| recover:  0.243| kl:  2.000| diverse:  0.046)\n",
      "Epochs: 05/100| It: 0018/0054 | Train loss:  2.501 (pred:  0.299| recover:  0.159| kl:  2.000| diverse:  0.043)\n",
      "Epochs: 05/100| It: 0036/0054 | Train loss:  2.495 (pred:  0.295| recover:  0.159| kl:  2.000| diverse:  0.041)\n",
      "Epochs: 06/100| It: 0000/0054 | Train loss:  2.402 (pred:  0.249| recover:  0.121| kl:  2.000| diverse:  0.033)\n",
      "Epochs: 06/100| It: 0018/0054 | Train loss:  2.574 (pred:  0.394| recover:  0.124| kl:  2.000| diverse:  0.057)\n",
      "Epochs: 06/100| It: 0036/0054 | Train loss:  2.629 (pred:  0.414| recover:  0.156| kl:  2.000| diverse:  0.059)\n",
      "Epochs: 07/100| It: 0000/0054 | Train loss:  2.717 (pred:  0.491| recover:  0.156| kl:  2.000| diverse:  0.070)\n",
      "Epochs: 07/100| It: 0018/0054 | Train loss:  2.523 (pred:  0.363| recover:  0.109| kl:  2.000| diverse:  0.050)\n",
      "Epochs: 07/100| It: 0036/0054 | Train loss:  2.673 (pred:  0.479| recover:  0.126| kl:  2.000| diverse:  0.068)\n",
      "Epochs: 08/100| It: 0000/0054 | Train loss:  2.465 (pred:  0.323| recover:  0.099| kl:  2.000| diverse:  0.042)\n",
      "Epochs: 08/100| It: 0018/0054 | Train loss:  2.602 (pred:  0.439| recover:  0.102| kl:  2.000| diverse:  0.061)\n",
      "Epochs: 08/100| It: 0036/0054 | Train loss:  2.744 (pred:  0.548| recover:  0.118| kl:  2.000| diverse:  0.078)\n",
      "Epochs: 09/100| It: 0000/0054 | Train loss:  2.451 (pred:  0.335| recover:  0.071| kl:  2.000| diverse:  0.045)\n",
      "Epochs: 09/100| It: 0018/0054 | Train loss:  2.567 (pred:  0.421| recover:  0.089| kl:  2.000| diverse:  0.057)\n",
      "Epochs: 09/100| It: 0036/0054 | Train loss:  2.595 (pred:  0.448| recover:  0.085| kl:  2.000| diverse:  0.062)\n",
      "Epochs: 10/100| It: 0000/0054 | Train loss:  2.538 (pred:  0.416| recover:  0.065| kl:  2.000| diverse:  0.057)\n",
      "Epochs: 10/100| It: 0018/0054 | Train loss:  2.410 (pred:  0.310| recover:  0.060| kl:  2.000| diverse:  0.039)\n",
      "Epochs: 10/100| It: 0036/0054 | Train loss:  2.626 (pred:  0.477| recover:  0.085| kl:  2.000| diverse:  0.064)\n",
      "Epochs: 11/100| It: 0000/0054 | Train loss:  2.472 (pred:  0.371| recover:  0.050| kl:  2.000| diverse:  0.051)\n",
      "Epochs: 11/100| It: 0018/0054 | Train loss:  2.502 (pred:  0.397| recover:  0.052| kl:  2.000| diverse:  0.053)\n",
      "Epochs: 11/100| It: 0036/0054 | Train loss:  2.440 (pred:  0.347| recover:  0.048| kl:  2.000| diverse:  0.045)\n",
      "Epochs: 12/100| It: 0000/0054 | Train loss:  2.380 (pred:  0.277| recover:  0.069| kl:  2.000| diverse:  0.034)\n",
      "Epochs: 12/100| It: 0018/0054 | Train loss:  2.434 (pred:  0.329| recover:  0.062| kl:  2.000| diverse:  0.043)\n",
      "Epochs: 12/100| It: 0036/0054 | Train loss:  2.460 (pred:  0.376| recover:  0.035| kl:  2.000| diverse:  0.049)\n",
      "Epochs: 13/100| It: 0000/0054 | Train loss:  2.669 (pred:  0.526| recover:  0.073| kl:  2.000| diverse:  0.070)\n",
      "Epochs: 13/100| It: 0018/0054 | Train loss:  2.594 (pred:  0.485| recover:  0.046| kl:  2.000| diverse:  0.062)\n",
      "Epochs: 13/100| It: 0036/0054 | Train loss:  2.318 (pred:  0.253| recover:  0.038| kl:  2.000| diverse:  0.028)\n",
      "Epochs: 14/100| It: 0000/0054 | Train loss:  2.629 (pred:  0.495| recover:  0.070| kl:  2.000| diverse:  0.063)\n",
      "Epochs: 14/100| It: 0018/0054 | Train loss:  2.401 (pred:  0.330| recover:  0.032| kl:  2.000| diverse:  0.039)\n",
      "Epochs: 14/100| It: 0036/0054 | Train loss:  2.437 (pred:  0.357| recover:  0.034| kl:  2.000| diverse:  0.046)\n",
      "Epochs: 15/100| It: 0000/0054 | Train loss:  2.564 (pred:  0.460| recover:  0.044| kl:  2.000| diverse:  0.059)\n",
      "Epochs: 15/100| It: 0018/0054 | Train loss:  2.395 (pred:  0.318| recover:  0.038| kl:  2.000| diverse:  0.039)\n",
      "Epochs: 15/100| It: 0036/0054 | Train loss:  2.301 (pred:  0.246| recover:  0.027| kl:  2.000| diverse:  0.028)\n",
      "Epochs: 16/100| It: 0000/0054 | Train loss:  2.312 (pred:  0.256| recover:  0.025| kl:  2.000| diverse:  0.031)\n",
      "Epochs: 16/100| It: 0018/0054 | Train loss:  2.426 (pred:  0.343| recover:  0.043| kl:  2.000| diverse:  0.040)\n",
      "Epochs: 16/100| It: 0036/0054 | Train loss:  2.489 (pred:  0.405| recover:  0.034| kl:  2.000| diverse:  0.050)\n",
      "Epochs: 17/100| It: 0000/0054 | Train loss:  2.511 (pred:  0.436| recover:  0.020| kl:  2.000| diverse:  0.055)\n",
      "Epochs: 17/100| It: 0018/0054 | Train loss:  2.286 (pred:  0.240| recover:  0.020| kl:  2.000| diverse:  0.026)\n",
      "Epochs: 17/100| It: 0036/0054 | Train loss:  2.510 (pred:  0.430| recover:  0.030| kl:  2.000| diverse:  0.050)\n",
      "Epochs: 18/100| It: 0000/0054 | Train loss:  2.408 (pred:  0.346| recover:  0.022| kl:  2.000| diverse:  0.040)\n",
      "Epochs: 18/100| It: 0018/0054 | Train loss:  2.485 (pred:  0.408| recover:  0.030| kl:  2.000| diverse:  0.048)\n",
      "Epochs: 18/100| It: 0036/0054 | Train loss:  2.484 (pred:  0.402| recover:  0.032| kl:  2.000| diverse:  0.051)\n",
      "Epochs: 19/100| It: 0000/0054 | Train loss:  2.392 (pred:  0.330| recover:  0.025| kl:  2.000| diverse:  0.037)\n",
      "Epochs: 19/100| It: 0018/0054 | Train loss:  2.434 (pred:  0.369| recover:  0.022| kl:  2.000| diverse:  0.044)\n",
      "Epochs: 19/100| It: 0036/0054 | Train loss:  2.470 (pred:  0.404| recover:  0.019| kl:  2.000| diverse:  0.047)\n",
      "Epochs: 20/100| It: 0000/0054 | Train loss:  2.428 (pred:  0.365| recover:  0.020| kl:  2.000| diverse:  0.043)\n",
      "Epochs: 20/100| It: 0018/0054 | Train loss:  2.329 (pred:  0.285| recover:  0.016| kl:  2.000| diverse:  0.029)\n",
      "Epochs: 20/100| It: 0036/0054 | Train loss:  2.608 (pred:  0.523| recover:  0.020| kl:  2.000| diverse:  0.066)\n",
      "Epochs: 21/100| It: 0000/0054 | Train loss:  2.430 (pred:  0.375| recover:  0.015| kl:  2.000| diverse:  0.040)\n",
      "Epochs: 21/100| It: 0018/0054 | Train loss:  2.528 (pred:  0.457| recover:  0.019| kl:  2.000| diverse:  0.052)\n",
      "Epochs: 21/100| It: 0036/0054 | Train loss:  2.479 (pred:  0.411| recover:  0.021| kl:  2.000| diverse:  0.046)\n",
      "Epochs: 22/100| It: 0000/0054 | Train loss:  2.436 (pred:  0.377| recover:  0.018| kl:  2.000| diverse:  0.041)\n",
      "Epochs: 22/100| It: 0018/0054 | Train loss:  2.447 (pred:  0.388| recover:  0.013| kl:  2.000| diverse:  0.046)\n",
      "Epochs: 22/100| It: 0036/0054 | Train loss:  2.489 (pred:  0.418| recover:  0.024| kl:  2.000| diverse:  0.047)\n",
      "Epochs: 23/100| It: 0000/0054 | Train loss:  2.389 (pred:  0.338| recover:  0.015| kl:  2.000| diverse:  0.036)\n",
      "Epochs: 23/100| It: 0018/0054 | Train loss:  2.364 (pred:  0.306| recover:  0.023| kl:  2.000| diverse:  0.034)\n",
      "Epochs: 23/100| It: 0036/0054 | Train loss:  2.440 (pred:  0.381| recover:  0.015| kl:  2.000| diverse:  0.043)\n",
      "Epochs: 24/100| It: 0000/0054 | Train loss:  2.561 (pred:  0.481| recover:  0.026| kl:  2.000| diverse:  0.054)\n",
      "Epochs: 24/100| It: 0018/0054 | Train loss:  2.630 (pred:  0.543| recover:  0.022| kl:  2.000| diverse:  0.064)\n",
      "Epochs: 24/100| It: 0036/0054 | Train loss:  2.355 (pred:  0.310| recover:  0.012| kl:  2.000| diverse:  0.033)\n",
      "Epochs: 25/100| It: 0000/0054 | Train loss:  2.382 (pred:  0.336| recover:  0.012| kl:  2.000| diverse:  0.034)\n",
      "Epochs: 25/100| It: 0018/0054 | Train loss:  2.340 (pred:  0.299| recover:  0.012| kl:  2.000| diverse:  0.029)\n",
      "Epochs: 25/100| It: 0036/0054 | Train loss:  2.520 (pred:  0.453| recover:  0.016| kl:  2.000| diverse:  0.050)\n",
      "Epochs: 26/100| It: 0000/0054 | Train loss:  2.451 (pred:  0.395| recover:  0.015| kl:  2.000| diverse:  0.042)\n",
      "Epochs: 26/100| It: 0018/0054 | Train loss:  2.366 (pred:  0.320| recover:  0.013| kl:  2.000| diverse:  0.033)\n",
      "Epochs: 26/100| It: 0036/0054 | Train loss:  2.357 (pred:  0.312| recover:  0.015| kl:  2.000| diverse:  0.030)\n",
      "Epochs: 27/100| It: 0000/0054 | Train loss:  2.393 (pred:  0.340| recover:  0.019| kl:  2.000| diverse:  0.034)\n",
      "Epochs: 27/100| It: 0018/0054 | Train loss:  2.550 (pred:  0.479| recover:  0.018| kl:  2.000| diverse:  0.053)\n",
      "Epochs: 27/100| It: 0036/0054 | Train loss:  2.471 (pred:  0.408| recover:  0.018| kl:  2.000| diverse:  0.045)\n",
      "Epochs: 28/100| It: 0000/0054 | Train loss:  2.430 (pred:  0.374| recover:  0.019| kl:  2.000| diverse:  0.037)\n",
      "Epochs: 28/100| It: 0018/0054 | Train loss:  2.472 (pred:  0.414| recover:  0.013| kl:  2.000| diverse:  0.045)\n",
      "Epochs: 28/100| It: 0036/0054 | Train loss:  2.343 (pred:  0.302| recover:  0.013| kl:  2.000| diverse:  0.029)\n",
      "Epochs: 29/100| It: 0000/0054 | Train loss:  2.302 (pred:  0.266| recover:  0.011| kl:  2.000| diverse:  0.025)\n",
      "Epochs: 29/100| It: 0018/0054 | Train loss:  2.465 (pred:  0.407| recover:  0.014| kl:  2.000| diverse:  0.044)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ba60cc655367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_continue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\" save model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_save_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-66cd139e4a66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0miter_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_recover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_diverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0;34m\"\"\" optimize \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GroupNet/.env/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GroupNet/model/GroupNet_dance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# z = pz_sampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mdiverse_pred_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_feature_repeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpz_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0mloss_diverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss_diverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiverse_pred_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_pred\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_recover\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_kl\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mloss_diverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GroupNet/.env/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GroupNet/model/GroupNet_dance.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, past_feature, z, batch_size_curr, agent_num_perscene, past_traj, cur_location, sample_num, mode)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# norm_seq = norm_seq.permute(2,0,1,3).view(self.future_length, agent_num * sample_num,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mcur_location_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_location\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mout_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcur_location_repeat\u001b[0m \u001b[0;31m# (agent_num*sample_num,self.past_length,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'inference'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" start training \"\"\"\n",
    "model.set_device(device)\n",
    "for epoch in range(args.epoch_continue, args.num_epochs):\n",
    "    train(train_loader,epoch)\n",
    "    \"\"\" save model \"\"\"\n",
    "    if  (epoch + 1) % args.model_save_epoch == 0:\n",
    "        model_saved = {'model_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'scheduler': scheduler.state_dict(), 'epoch': epoch + 1,'model_cfg': args}\n",
    "        saved_path = os.path.join(args.model_save_dir,str(epoch+1)+'.p')\n",
    "        torch.save(model_saved, saved_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
